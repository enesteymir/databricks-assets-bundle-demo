resources:
  jobs:
    cdp_wheel_job_demo:
      name: T-Rex CDP Wheel Job Demo

      schedule:
        quartz_cron_expression: "0 0 0 * * ? *"
        timezone_id: Europe/Berlin

      # email_notifications:
      #   on_failure: ${var.default_failure_notifications}

      tasks:
        - task_key: main_task
          job_cluster_key: cdp_wheel_job_cluster_key
          python_wheel_task:
            package_name: cdp_demo                  # the name of the project in pyproject.toml
            entry_point: cdp_wheel_job_entrypoint   # the entry_point from the project.scripts in pyproject.toml
            named_parameters: { env: "${var.environment}", day: "${var.day}" }
          libraries:
            - whl: ../dist/*.whl
            - pypi:
                package: anomalo==0.23.0


      job_clusters:
        - job_cluster_key: cdp_wheel_job_cluster_key
          new_cluster:
            spark_version: 14.3.x-scala2.12
            node_type_id: c-fleet.xlarge
            runtime_engine: STANDARD
            enable_elastic_disk: true
            autoscale:
                min_workers: 1
                max_workers: 1
            aws_attributes:
              availability: "SPOT_WITH_FALLBACK"
              instance_profile_arn: "arn:aws:iam::728296428228:instance-profile/team-tracking-cluster-role"
              zone_id: "auto"
              first_on_demand: 1
              spot_bid_price_percent: 70
            policy_id: ${var.base_cluster_policy_id}
            custom_tags:
              cost_allocation: ${var.cost_allocation_id}
            spark_conf:
              "spark.sql.parquet.compression.codec": "zstd"               # Enables efficient storage compression, highly recommended.
              "spark.databricks.delta.schema.autoMerge.enabled": "true"   # Allows automatic merging of schema changes during writes to Delta tables.
              "spark.databricks.delta.autoCompact.enabled": "auto"        # Combines small files within Delta table partitions to automatically reduce small file problems, after each write
              "spark.databricks.delta.optimizeWrite.enabled": "true"      # Improves the target file size while writing, increase read performance. But can increase write latency and memory usage.